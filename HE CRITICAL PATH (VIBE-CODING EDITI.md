HE CRITICAL PATH (VIBE-CODING EDITION)

Your goal:
Get the whole pipeline working end-to-end ONCE â€” ugly, messy, no refactor â€” and measure latency.

You are NOT building the app.
You are proving the app can be built.

This is the only thing that matters.

We will do it in 5 micro-milestones.

DAY 1 â€” Whisper Loop

Goal:
You speak â†’ Whisper tiny.en (CPU) â†’ Text appears in terminal â†’ Under 1.4s.

TODO:

Create a new file called:
01_whisper_test.py

Use faster-whisper:

stt = WhisperModel("tiny.en", device="cpu", compute_type="int8")


Record exactly 1.2 seconds of audio using sounddevice:

Fixed 16000 Hz

Mono

Pass that raw PCM audio directly into Whisper.

Print the text and the total latency.

What NOT to do:

No GUI

No VAD

No multiprocessing

No embeddings

No queues

Success Criteria:

1.2sâ€“1.4s consistent transcribe time on your slowest laptop

Text isnâ€™t garbage

Script finishes without errors

DAY 2 â€” Embedding + Intent Loop

Goal:
Take Whisper text â†’ MiniLM â†’ dot product â†’ Correct prompt returned.

TODO:

Create file:
02_intent_test.py

Load sentence-transformers MiniLM:

encoder = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')


Embed once:

Your prompt list (20â€“25 prompts)

Save vectors to .npy

Load your Whisper text from yesterday into this file.

Embed it.

Compute dot products vs prompt_matrix.

Print top 1 match.

What NOT to do:

No UI

No VAD

No async

No queues

No batching

No normalization

Success Criteria:

Embedding time < 80ms

Dot product < 5ms

Correct prompt shown

DAY 3 â€” The Sentinel (VAD + Buffer Loop)

Goal:
Detect speech â†’ Detect silence â†’ Grab last 1.2s audio â†’ Print â€œTRIGGERâ€.

TODO:

Create file:
03_sentinel_test.py

Use sounddevice stream:

16000 Hz

Blocksize = 320 samples (20ms)

Use Silero VAD:

model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad')


Build a ring buffer holding last 1200ms of audio (60 frames).

Logic:

VAD > 0.5 â†’ speaking

VAD < 0.3 for 0.6s â†’ silence

When silence hits:

Dump the ring buffer to a WAV/PCM file

Print â€œSILENCE DETECTED â€“ AUDIO SAVEDâ€

What NOT to do:

No queues

No Whisper

No embeddings

Success Criteria:

Silence reliably detected

No jitter

No dropped frames

Ring buffer extraction works

DAY 4 â€” Integrate Sentinel â†’ Worker (Single Process ONLY)

Goal:
Speak â†’ Silence â†’ Whisper â†’ MiniLM â†’ Prompt â†’ Print result.

THIS IS THE MVP MOMENT.

TODO:

Create file:
04_pipeline_test.py

Copy your Day 3 sentinel code.

Inside the silence event:

Call Whisper immediately

Call embedding

Call dot product

Print resulting prompt

Add timer:

start = time.time()
...
print("Latency:", time.time() - start)


What NOT to do:

No multiprocessing

No UI

No governor

No background threads

No async

Success Criteria:

Speak

Pause

<1.5s later: prompt prints in terminal

Run 20 times â†’ 90%+ success

This is your OMTM moment.

DAY 5 â€” Add the UI (Still Single Process)

Goal:
Replace â€œprint promptâ€ with UI overlay text.

TODO:

Create file:
05_ui_overlay_test.py

Use PyQt6 with transparent window.

On silence event:

Run Whisper â†’ MiniLM â†’ prompt

Set label text

Show window

Hide after 3s

Test under heavy CPU load (Zoom ON).

What NOT to do:

No multiprocessing

No queues

No threading (except Qt)

No â€œgovernorâ€ logic yet

Success Criteria:

UI shows the prompt

No stutter

No lag

Window stays on top

Does not steal focus

ðŸš¨ AFTER ALL 5 ARE WORKING â€” DEADLINE

Only THEN do you go:

â†’ Multiprocessing
â†’ Queues
â†’ Governor
â†’ UX polish
â†’ Installer
â†’ Auto-start
â†’ Wizard
â†’ Deployment

Right now, your ONLY job is to:

Build a single-process prototype that proves end-to-end latency:
SILENCE â†’ TRANSCRIBE â†’ EMBED â†’ PROMPT under 1.5 seconds.

If that works, the MVP works.
If that fails, you fix Whisper OR your buffer â€” NOT the architecture.